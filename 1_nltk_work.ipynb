{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% libraries\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import io\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "# run for jupyter notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/alal/LAL_DATA/Newspapers/The Himalayan Times'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = '/media/alal/LAL_DATA/Newspapers/The Himalayan Times/'\n",
    "os.chdir(root)\n",
    "%pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick file, remove punctuation and stopwords\n",
    "tmp = '/home/alal/tmp'\n",
    "inp = root + '/raw_txts'\n",
    "out = root + '/word_frequencies/'\n",
    "\n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_word_freqs(inputfile,outdir):\n",
    "    filterout= set(stopwords.words('english')+\n",
    "               list(string.punctuation)+\n",
    "               ['\\'\\'','``','\\'s','’',\"“\",\"”\",\n",
    "                'the','said','nepal','world','kathmandu'])\n",
    "    cols = ['word','freq']\n",
    "\n",
    "    base = os.path.abspath(inputfile)\n",
    "    wdir, fname = outdir, os.path.split(base)[1]\n",
    "    writepath = wdir + '/wfreqs_' + fname.split('.')[0] + '.csv'\n",
    "\n",
    "    f = open(inputfile)\n",
    "    raw = f.read()\n",
    "    tokens = [token.lower() for token in nltk.word_tokenize(raw)]\n",
    "    cleaned = [token for token in tokens if token not in filterout]\n",
    "    \n",
    "    fdict = dict(nltk.FreqDist(cleaned))\n",
    "    df = pd.DataFrame(list(fdict.items()),columns=cols)\n",
    "    df = df.sort_values('freq',ascending=0)\n",
    "    \n",
    "    df.to_csv(writepath,columns=['word','freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sentence splitter relies on nltk data**\n",
    "\n",
    "run \n",
    "```{python}\n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "and select 'all packages' (3 GB download to `/home/<user>/` ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('/media/alal/LAL_DATA/Newspapers/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_sentences(inputfile,outdir):\n",
    "    base = os.path.abspath(inputfile)\n",
    "    wdir, fname = outdir, os.path.split(base)[1]\n",
    "    writepath = wdir + '/sentences_' + fname.split('.')[0] + '.txt'\n",
    "\n",
    "    f = open(inputfile)\n",
    "    raw = f.read()\n",
    "    string = raw.replace('\\n',\" \")\n",
    "    sentences = [token.lower() for token in nltk.tokenize.sent_tokenize(string)]\n",
    "\n",
    "    outF = open(writepath, \"w\")\n",
    "    sentences = map(lambda x: x+\"\\n\", sentences)\n",
    "\n",
    "    outF.writelines(sentences)\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize execution of word-counter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick file, remove punctuation and stopwords\n",
    "tmp = '/home/alal/tmp'\n",
    "inp = root + 'raw_txts'\n",
    "out = root + '/word_frequencies/'\n",
    "\n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(inp+'/THT_*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 722 ms, sys: 101 ms, total: 823 ms\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = Parallel(n_jobs=num_cores)(delayed(write_word_freqs)(i,out) \\\n",
    "                                     for i in files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize execution of sentence splitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick file, remove punctuation and stopwords\n",
    "tmp = '/home/alal/tmp'\n",
    "inp = root + 'raw_txts'\n",
    "out = root + '/sentences/'\n",
    "\n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(inp+'/THT_*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 297 ms, sys: 63.3 ms, total: 360 ms\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = Parallel(n_jobs=num_cores)(delayed(write_sentences)(i,out) \\\n",
    "                                     for i in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
